Project Overview
- Objective
- Data Source
- Tools
- Result

>> image (dataset structure)

---------------

airflow:
- set up a scheduler
- grouping tasks into sections
- chaining together tasks
- establish connection, use env variables + airflow variables from db
- use cosmos dbt 
- use astro instead of docker compose with basic airflow images
- using operators for different services (dbt cosmos, bigquery, bucket)


python code:
- request for getting data from api
- pandas for data transformations 

deployment:
- astro via github integration

---------------

---------------

---------------


---------------
images:
- final tables schema 
- airflow DAG structure, astro ui (2-4 images)